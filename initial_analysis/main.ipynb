{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import sys\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Load Tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_train = pd.read_fwf('data/tweets/train_neg.txt', header=None, names=['tweets'])\n",
    "pos_train = pd.read_fwf('data/tweets/train_pos.txt', header=None, names=['tweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Clean Tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load stop words\n",
    "f = open(\"data/stop_words.txt\", 'r')\n",
    "stop_words = list()\n",
    "for word in f:\n",
    "    if word != '\\n':\n",
    "        stop_words += [word.strip(\"\\n\").strip(\" \").strip(\"\\t\")]\n",
    "f.close()\n",
    "\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    \n",
    "    cleaned_tweet = str()\n",
    "    for word in tweet.lower().split():\n",
    "            \n",
    "        # remove non-latin characters\n",
    "        clean_word = regex.sub(u'[^\\p{Latin}]', u'', word)\n",
    "        \n",
    "        # don't include stop words\n",
    "        if clean_word not in stop_words:\n",
    "            cleaned_tweet += clean_word + \" \"\n",
    "    \n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean tweets\n",
    "neg_train_clean = neg_train['tweets'].apply(cleanTweet)\n",
    "pos_train_clean = pos_train['tweets'].apply(cleanTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Load Word Embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note that length of word embeddings for Word2vec and Fasttext is 300 and 200 for Glove.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Word2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\"data/embeddings/word2vec.en.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fasttext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fasttext_model = KeyedVectors.load_word2vec_format(\"data/embeddings/fasttext.en.vec\",\\\n",
    "                                                   binary=False, limit=100000)\n",
    "# fasttext_model = FastText.load_fasttext_format('data/embeddings/fasttext.en')\n",
    "# fasttext_model = fasttext.load_model(\"data/embeddings/fasttext.1.en.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Glove`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    return model\n",
    "\n",
    "glove_model = loadGloveModel(\"data/embeddings/glove/glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pre-processing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = pos_train_clean.values\n",
    "neg_tweets = neg_train_clean.values\n",
    "\n",
    "num_samples = 100000\n",
    "pos_labels = num_samples * [1] \n",
    "neg_labels = num_samples * [0] \n",
    "\n",
    "train_sentences = np.array(list(pos_tweets) + list(neg_tweets))\n",
    "train_labels = np.array(pos_labels + neg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average all word-embeddings within a sentence\n",
    "def average_embeddings(sentences, model, vector_length):\n",
    "    return_matrix = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        word_count = float(len(words))\n",
    "        temp_list = np.zeros(vector_length)\n",
    "        for word in words:\n",
    "            try:\n",
    "                temp_list += model[word][:vector_length]\n",
    "            except:\n",
    "                word_count -= 1\n",
    "        if word_count == 0:\n",
    "            avrg_list = np.zeros(vector_length)\n",
    "        else:\n",
    "            avrg_list = temp_list/word_count\n",
    "        return_matrix.append(avrg_list)\n",
    "    return return_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert tweets into bag-of-words representations\n",
    "vector_length = 200\n",
    "train_features = average_embeddings(train_sentences, glove_model, vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Predict Categories with Scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Logistic Regression Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(C=1e5)\n",
    "lr.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75675000000000003"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_labels, lr.predict(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `Model` | `Accuracy`   |\n",
    "|------|------|\n",
    "| *`Sampled`* `Fasttext` | `0.72694`|\n",
    "| `Word2vec` | `0.72697`|\n",
    "| `Glove` | `0.75675`|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
