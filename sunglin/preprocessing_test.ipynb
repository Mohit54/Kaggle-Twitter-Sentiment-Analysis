{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Copyright 2017, Dimitrios Effrosynidis, All rights reserved. \"\"\"\n",
    "\n",
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from time import time\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def replaceMultiExclamationMark(text):\n",
    "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
    "    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
    "    return text\n",
    "\n",
    "def replaceMultiQuestionMark(text):\n",
    "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
    "    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
    "    return text\n",
    "\n",
    "def replaceMultiStopMark(text):\n",
    "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
    "    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def replaceElongated(word):\n",
    "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
    "\n",
    "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "    if repl_word != word:      \n",
    "        return replaceElongated(repl_word)\n",
    "    else:       \n",
    "        return repl_word\n",
    "\n",
    "def removeEmoticons(text):\n",
    "    \"\"\" Removes emoticons from text \"\"\"\n",
    "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "    return text\n",
    "\n",
    "### Spell Correction begin ###\n",
    "\"\"\" Spell Correction http://norvig.com/spell-correct.html \"\"\"\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../data/preprocessing/corporaForSpellCorrection.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"\"\"P robability of `word`. \"\"\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def spellCorrection(word): \n",
    "    \"\"\" Most probable spelling correction for word. \"\"\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"\"\" Generate possible spelling corrections for word. \"\"\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"\"\" The subset of `words` that appear in the dictionary of WORDS. \"\"\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\" All edits that are one edit away from `word`. \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"\"\" All edits that are two edits away from `word`. \"\"\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "### Spell Correction End ###\n",
    "\n",
    "### Replace Negations Begin ###\n",
    "\n",
    "def replace(word, pos=None):\n",
    "    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n",
    "    antonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                  antonyms.add(antonym.name())\n",
    "    if len(antonyms) == 1:\n",
    "        return antonyms.pop()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def replaceNegations(text):\n",
    "    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n",
    "    i, l = 0, len(text)\n",
    "    words = []\n",
    "    while i < l:\n",
    "        word = text[i]\n",
    "        if word == 'not' and i+1 < l:\n",
    "            ant = replace(text[i+1])\n",
    "            if ant:\n",
    "                words.append(ant)\n",
    "                i += 2\n",
    "                continue\n",
    "        words.append(word)\n",
    "        i += 1\n",
    "    return words\n",
    "\n",
    "### Replace Negations End ###\n",
    "\n",
    "def addNotTag(text):\n",
    "    \"\"\" Finds \"not,never,no\" and adds the tag NEG_ to all words that follow until the next punctuation \"\"\"\n",
    "    transformed = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s]+[^\\w\\s]', \n",
    "       lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)), \n",
    "       text,\n",
    "       flags=re.IGNORECASE)\n",
    "    return transformed\n",
    "\n",
    "def addCapTag(word):\n",
    "    \"\"\" Finds \"not,never,no\" and adds the tag ALL_CAPS_ to all words that follow until the next punctuation \"\"\"\n",
    "    if(len(re.findall(\"[A-Z]{3,}\", word))):\n",
    "        word = word.replace('\\\\', '' )\n",
    "        transformed = re.sub(\"[A-Z]{3,}\", \"ALL_CAPS_\"+word, word)\n",
    "        return transformed\n",
    "    else:\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "# my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm\" # my extra stopwords\n",
    "stoplist = stoplist\n",
    "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
    "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "stemmer = PorterStemmer() # set stemmer\n",
    "\n",
    "\"\"\" Creates a dictionary with slangs and their equivalents and replaces them \"\"\"\n",
    "with open('../data/preprocessing/slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2]) for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    sentence = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens = replaceNegations(tokens) # Technique 6: finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator) # Technique 7: remove punctuation\n",
    "\n",
    "    tokens = nltk.word_tokenize(text) # it takes a text as an input and provides a list of every token in it\n",
    "                \n",
    "    tagged = nltk.pos_tag(tokens) # Technique 13: part of speech tagging  \n",
    "    \n",
    "    for w in tagged:\n",
    "        if (w[1][0] in allowedWordTypes and w[0] not in stoplist):\n",
    "            final_word = addCapTag(w[0])\n",
    "            #final_word = final_word.lower()\n",
    "            final_word = replaceElongated(final_word)\n",
    "            if len(final_word)>1:\n",
    "                final_word = spellCorrection(final_word)\n",
    "            final_word = lemmatizer.lemmatize(final_word)\n",
    "            final_word = stemmer.stem(final_word) \n",
    "            sentence.append(final_word)\n",
    "        \n",
    "    return sentence\n",
    "\n",
    "def cleanTweet(text):\n",
    "    text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "    text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "    text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "    text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation marks with the tag \"multiExclamation\"\n",
    "    text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks with the tag \"multiQuestion\"\n",
    "    text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks with the tag \"multiStop\"\n",
    "    \n",
    "    tokens = tokenize(text) \n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
