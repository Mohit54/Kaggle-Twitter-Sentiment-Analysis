{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import sys\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Load Tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Copyright 2017, Dimitrios Effrosynidis, All rights reserved. \"\"\"\n",
    "\n",
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from time import time\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def replaceMultiExclamationMark(text):\n",
    "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
    "    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
    "    return text\n",
    "\n",
    "def replaceMultiQuestionMark(text):\n",
    "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
    "    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
    "    return text\n",
    "\n",
    "def replaceMultiStopMark(text):\n",
    "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
    "    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def replaceElongated(word):\n",
    "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
    "\n",
    "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "    if repl_word != word:      \n",
    "        return replaceElongated(repl_word)\n",
    "    else:       \n",
    "        return repl_word\n",
    "\n",
    "def removeEmoticons(text):\n",
    "    \"\"\" Removes emoticons from text \"\"\"\n",
    "    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n",
    "    return text\n",
    "\n",
    "### Spell Correction begin ###\n",
    "\"\"\" Spell Correction http://norvig.com/spell-correct.html \"\"\"\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../data/preprocessing/corporaForSpellCorrection.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"\"\"P robability of `word`. \"\"\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def spellCorrection(word): \n",
    "    \"\"\" Most probable spelling correction for word. \"\"\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"\"\" Generate possible spelling corrections for word. \"\"\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"\"\" The subset of `words` that appear in the dictionary of WORDS. \"\"\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\" All edits that are one edit away from `word`. \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"\"\" All edits that are two edits away from `word`. \"\"\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "### Spell Correction End ###\n",
    "\n",
    "### Replace Negations Begin ###\n",
    "\n",
    "def replace(word, pos=None):\n",
    "    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n",
    "    antonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                  antonyms.add(antonym.name())\n",
    "    if len(antonyms) == 1:\n",
    "        return antonyms.pop()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def replaceNegations(text):\n",
    "    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n",
    "    i, l = 0, len(text)\n",
    "    words = []\n",
    "    while i < l:\n",
    "        word = text[i]\n",
    "        if word == 'not' and i+1 < l:\n",
    "            ant = replace(text[i+1])\n",
    "            if ant:\n",
    "                words.append(ant)\n",
    "                i += 2\n",
    "                continue\n",
    "        words.append(word)\n",
    "        i += 1\n",
    "    return words\n",
    "\n",
    "### Replace Negations End ###\n",
    "\n",
    "def addNotTag(text):\n",
    "    \"\"\" Finds \"not,never,no\" and adds the tag NEG_ to all words that follow until the next punctuation \"\"\"\n",
    "    transformed = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s]+[^\\w\\s]', \n",
    "       lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)), \n",
    "       text,\n",
    "       flags=re.IGNORECASE)\n",
    "    return transformed\n",
    "\n",
    "def addCapTag(word):\n",
    "    \"\"\" Finds \"not,never,no\" and adds the tag ALL_CAPS_ to all words that follow until the next punctuation \"\"\"\n",
    "    if(len(re.findall(\"[A-Z]{3,}\", word))):\n",
    "        word = word.replace('\\\\', '' )\n",
    "        transformed = re.sub(\"[A-Z]{3,}\", \"ALL_CAPS_\"+word, word)\n",
    "        return transformed\n",
    "    else:\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_train = pd.read_fwf('../data/tweets/train_neg.txt', header=None, names=['tweets'])\n",
    "pos_train = pd.read_fwf('../data/tweets/train_pos.txt', header=None, names=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tresorpack 6 ( difficulty 10 of 10 objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad i dot have taks tomorrow ! ! #thankful #s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-3 vs celtics in the regular season = were fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i could actually kill that girl i'm so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; i find that very hard to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wish i could be out all night tonight ! &lt;user&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; i got kicked out the wgm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rt &lt;user&gt; &lt;user&gt; &lt;user&gt; yes she is ! u tell it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>why is she so perfect &lt;url&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;user&gt; hi harry ! did u havea good time in aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>introduction to programming with c + + ( 2nd e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>introduction to programming with c + + ( 2nd e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>introduction to programming with c + + ( 2nd e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;user&gt; i'm white . #aw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;user&gt; dan i love and miss you ! don't be sad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>so many wonderful building in dc but still mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;user&gt; it's annoying because i secretly find i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>the post-boom in spanish american fiction ( su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>layers of the heart ( paperback this journey w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>layers of the heart ( paperback this journey w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>guess who texted me again and wants us back ( ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;user&gt; # 2 farrow ( v a litter offence ; wilfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>99000 people at barca's ground tonight are gon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mlb 07 the show ( video game mlb 07 : the show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rt if #justinbieber is not following you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rt if #justinbieber is not following you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rt if #justinbieber is not following you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>why should i even care ? she dont want my whit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ugh idk why i got my hopes up ! ! ! my plans n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;user&gt; ummm playing trials hd and ignoring the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>trying to get &lt;user&gt; tickets for the vancouver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>04x27 custom picture frame / poster frame 1.5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sooo i missed music class ! does this mean ! ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;user&gt; not when i'm straight and i'm taller th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; bloodforge not worth fighting to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>i really want him to do something special and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bye bye beach and buddies ( @ fort lauderdale-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>damn it , just saw a spoiler that told me how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>just realized i still don't know what my prom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>marion zimmer bradley's ravens of avalon [ wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>why do i get treated like this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;user&gt; they're the best cookies that i've trie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;user&gt; haha afraid not stir fry veg and chicken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;user&gt; we always have finals on saturdays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[ rivals ] &lt;url&gt; war room 4/23 ( rivals david ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;user&gt; ow im very sorry : ' - ( but im not too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;user&gt; i'm not surprised you shouldn't do that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>all these people that meet &lt;user&gt; and get dmed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>suffering from post-festival disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>&lt;user&gt; their not getting divorced !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweets\n",
       "0   vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
       "1   glad i dot have taks tomorrow ! ! #thankful #s...\n",
       "2   1-3 vs celtics in the regular season = were fu...\n",
       "3   <user> i could actually kill that girl i'm so ...\n",
       "4   <user> <user> <user> i find that very hard to ...\n",
       "5      wish i could be out all night tonight ! <user>\n",
       "6                     <user> i got kicked out the wgm\n",
       "7   rt <user> <user> <user> yes she is ! u tell it...\n",
       "8                         why is she so perfect <url>\n",
       "9   <user> hi harry ! did u havea good time in aus...\n",
       "10  introduction to programming with c + + ( 2nd e...\n",
       "11  introduction to programming with c + + ( 2nd e...\n",
       "12  introduction to programming with c + + ( 2nd e...\n",
       "13                             <user> i'm white . #aw\n",
       "14  <user> dan i love and miss you ! don't be sad ...\n",
       "15  so many wonderful building in dc but still mis...\n",
       "16  <user> it's annoying because i secretly find i...\n",
       "17  the post-boom in spanish american fiction ( su...\n",
       "18  layers of the heart ( paperback this journey w...\n",
       "19  layers of the heart ( paperback this journey w...\n",
       "20  guess who texted me again and wants us back ( ...\n",
       "21  <user> # 2 farrow ( v a litter offence ; wilfu...\n",
       "22  99000 people at barca's ground tonight are gon...\n",
       "23  mlb 07 the show ( video game mlb 07 : the show...\n",
       "24           rt if #justinbieber is not following you\n",
       "25           rt if #justinbieber is not following you\n",
       "26           rt if #justinbieber is not following you\n",
       "27  why should i even care ? she dont want my whit...\n",
       "28  ugh idk why i got my hopes up ! ! ! my plans n...\n",
       "29  <user> ummm playing trials hd and ignoring the...\n",
       "30  trying to get <user> tickets for the vancouver...\n",
       "31  04x27 custom picture frame / poster frame 1.5 ...\n",
       "32  sooo i missed music class ! does this mean ! ....\n",
       "33  <user> not when i'm straight and i'm taller th...\n",
       "34  <user> <user> bloodforge not worth fighting to...\n",
       "35  i really want him to do something special and ...\n",
       "36  bye bye beach and buddies ( @ fort lauderdale-...\n",
       "37  damn it , just saw a spoiler that told me how ...\n",
       "38  just realized i still don't know what my prom ...\n",
       "39  marion zimmer bradley's ravens of avalon [ wit...\n",
       "40                     why do i get treated like this\n",
       "41  <user> they're the best cookies that i've trie...\n",
       "42    <user> haha afraid not stir fry veg and chicken\n",
       "43          <user> we always have finals on saturdays\n",
       "44  [ rivals ] <url> war room 4/23 ( rivals david ...\n",
       "45  <user> ow im very sorry : ' - ( but im not too...\n",
       "46  <user> i'm not surprised you shouldn't do that...\n",
       "47  all these people that meet <user> and get dmed...\n",
       "48              suffering from post-festival disorder\n",
       "49                <user> their not getting divorced !"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_train.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Clean Tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "# my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm\" # my extra stopwords\n",
    "stoplist = stoplist\n",
    "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
    "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "stemmer = PorterStemmer() # set stemmer\n",
    "\n",
    "\"\"\" Creates a dictionary with slangs and their equivalents and replaces them \"\"\"\n",
    "with open('../data/preprocessing/slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2]) for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    sentence = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens = replaceNegations(tokens) # Technique 6: finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator) # Technique 7: remove punctuation\n",
    "\n",
    "    tokens = nltk.word_tokenize(text) # it takes a text as an input and provides a list of every token in it\n",
    "                \n",
    "    tagged = nltk.pos_tag(tokens) # Technique 13: part of speech tagging  \n",
    "    \n",
    "    for w in tagged:\n",
    "        if (w[1][0] in allowedWordTypes and w[0] not in stoplist):\n",
    "            final_word = addCapTag(w[0])\n",
    "            #final_word = final_word.lower()\n",
    "            final_word = replaceElongated(final_word)\n",
    "            if len(final_word)>1:\n",
    "                final_word = spellCorrection(final_word)\n",
    "            final_word = lemmatizer.lemmatize(final_word)\n",
    "            final_word = stemmer.stem(final_word) \n",
    "            sentence.append(final_word)\n",
    "        \n",
    "    return sentence\n",
    "\n",
    "def cleanTweet(text):\n",
    "    text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "    text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "    text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "    text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation marks with the tag \"multiExclamation\"\n",
    "    text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks with the tag \"multiQuestion\"\n",
    "    text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks with the tag \"multiStop\"\n",
    "    \n",
    "    tokens = tokenize(text) \n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tweets\n",
    "neg_train_clean = neg_train['tweets'].apply(cleanTweet)\n",
    "pos_train_clean = pos_train['tweets'].apply(cleanTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle.dump(neg_train_clean, open(\"../data/pickle/neg_train_clean.p\", 'wb'))\n",
    "# pickle.dump(pos_train_clean, open(\"../data/pickle/pos_train_clean.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg_train_clean = pickle.load(open(\"../data/pickle/neg_train_clean.p\", 'rb'))\n",
    "pos_train_clean = pickle.load(open(\"../data/pickle/pos_train_clean.p\", 'rb'))\n",
    "\n",
    "def clean_user(text):\n",
    "    text = text.replace(\"user\", \"\").replace(\"url\", \"\").replace(\"...\", \"\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_clean = neg_train_clean.apply(clean_user)\n",
    "pos_train_clean = pos_train_clean.apply(clean_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    into tresorpack difficulti object disassembl r...\n",
       "1                   glad dot take tomorrow thank start\n",
       "2        versu celtic regular season tuck play playoff\n",
       "3                               actual kill girl sorri\n",
       "4                           find hard believ in afraid\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_train_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Load Word Embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note that length of word embeddings for Word2vec and Fasttext is 300 and 200 for Glove.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Word2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\"data/embeddings/word2vec.en.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fasttext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = KeyedVectors.load_word2vec_format(\"../data/embeddings/crawl-300d-2M.vec\",\\\n",
    "                                                   binary=False, limit=100000)\n",
    "# fasttext_model = FastText.load_fasttext_format('data/embeddings/fasttext.en')\n",
    "# fasttext_model = fasttext.load_model(\"data/embeddings/fasttext.1.en.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Glove`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    return model\n",
    "\n",
    "glove_model = loadGloveModel(\"../data/embeddings/glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pre-processing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = pos_train_clean.values\n",
    "neg_tweets = neg_train_clean.values\n",
    "\n",
    "num_samples = 100000\n",
    "pos_labels = num_samples * [1] \n",
    "neg_labels = num_samples * [0] \n",
    "\n",
    "train_sentences = np.array(list(pos_tweets) + list(neg_tweets))\n",
    "train_labels = np.array(pos_labels + neg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average all word-embeddings within a sentence\n",
    "def average_embeddings(sentences, model, vector_length):\n",
    "    return_matrix = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        word_count = float(len(words))\n",
    "        temp_list = np.zeros(vector_length)\n",
    "        for word in words:\n",
    "            try:\n",
    "                temp_list += model[word][:vector_length]\n",
    "            except:\n",
    "                word_count -= 1\n",
    "        if word_count == 0:\n",
    "            avrg_list = np.zeros(vector_length)\n",
    "        else:\n",
    "            avrg_list = temp_list/word_count\n",
    "        return_matrix.append(avrg_list)\n",
    "    return return_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tweets into bag-of-words representations\n",
    "vector_length = 200\n",
    "train_features = average_embeddings(train_sentences, glove_model, vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Predict Categories with Scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Logistic Regression Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression(C=1e5)\n",
    "lr.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72583500000000001"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_labels, lr.predict(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `Model` | `Accuracy`   |\n",
    "|------|------|\n",
    "| *`Sampled`* `Fasttext` | `0.72694`|\n",
    "| `Word2vec` | `0.72697`|\n",
    "| `Glove` | `0.75675`|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
