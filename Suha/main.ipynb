{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import io\n",
    "import itertools\n",
    "import re\n",
    "from collections import Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from ekphrasis.classes import segmenter, spellcorrect\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import binascii\n",
    "import fasttext\n",
    "\n",
    "from fast_text import fast_text\n",
    "from glove_helper import load_glove_model, average_embeddings\n",
    "from preprocessing_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    data = []\n",
    "    file = open(filename, \"r\")\n",
    "    for line in file:\n",
    "        data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = pd.DataFrame(load_file('twitter-dataset/train_pos_full.txt'), columns=['tweet'])\n",
    "pos_tweets['sentiment'] = 1\n",
    "\n",
    "neg_tweets = pd.DataFrame(load_file('twitter-dataset/train_neg_full.txt'), columns=['tweet'])\n",
    "neg_tweets['sentiment'] = 0\n",
    "\n",
    "train_tweets = pd.concat([pos_tweets, neg_tweets], axis=0)\n",
    "test_tweets = train_tweets.sample(frac=0.01)\n",
    "train_tweets = train_tweets.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250000 train and 25000 test Tweet samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d train and %d test Tweet samples.\"%(len(train_tweets),len(test_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_methods(train_tweets, test_tweets, method_list):  \n",
    "    \n",
    "    train_tweets_copy = train_tweets.copy()\n",
    "    test_tweets_copy = test_tweets.copy()\n",
    "    \n",
    "    print(\"Pipeline < \" + \" > \".join([str(method).split()[1] for method in method_list]) + \"\\n\")\n",
    "    \n",
    "    for method in method_list:\n",
    "        print(\"Applying < %s\"%str(method).split()[1])\n",
    "        train_tweets_copy['tweet'] = train_tweets_copy['tweet'].apply(method)\n",
    "        test_tweets_copy['tweet'] = test_tweets_copy['tweet'].apply(method)\n",
    "    \n",
    "    train_tweets_copy['tweet'].replace('', np.nan, inplace=True)\n",
    "    test_tweets_copy['tweet'].replace('', np.nan, inplace=True)\n",
    "    train_tweets_copy = train_tweets_copy.dropna()\n",
    "    test_tweets_copy = test_tweets_copy.dropna()\n",
    "    \n",
    "    print(\"\\nDone!\")\n",
    "    return(train_tweets_copy, test_tweets_copy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# All Avaliable Pre-processing Methods\n",
    "# Choose the Ones to Apply\n",
    "all_methods = [standardize_repetitive, replace_slang, replace_contraction, replace_emoji, word_segmentation,\n",
    "              spell_correction, remove_punctiotions, emphasize_sentiment_words, filter_digits, is_word,\\\n",
    "              clean_stopwords, clean_one_length, clean_redundant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline < replace_contraction > replace_emoji > replace_slang > emphasize_sentiment_words > filter_digits\n",
      "\n",
      "Applying < replace_contraction\n",
      "Applying < replace_emoji\n",
      "Applying < replace_slang\n",
      "Applying < emphasize_sentiment_words\n",
      "Applying < filter_digits\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "selected_methods = [replace_contraction, replace_emoji, replace_slang, emphasize_sentiment_words, filter_digits]\n",
    "train_tweets_cleaned, test_tweets_cleaned  = grid_methods(train_tweets, test_tweets, selected_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text to Vector Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Internal Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy > 0.824720\n"
     ]
    }
   ],
   "source": [
    "pred = fast_text(train_tweets_cleaned, test_tweets_cleaned)\n",
    "pred = np.array(pred)\n",
    "acc_list = [1 if el else 0 for el in list(test_tweets_cleaned['sentiment']) == pred]\n",
    "acc = sum(acc_list) / len(acc_list)\n",
    "print(\"Accuracy > %f\"%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Word Embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the Model (just Once)\n",
    "fast_text.create_fasttext_train_file(train_tweets)\n",
    "fasttext.skipgram('fasttext_train.txt', 'fasttext_model', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.load_model('embeddings/fasttext_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = list(train_tweets_cleaned['tweet'])\n",
    "test_sentences = list(test_tweets_cleaned['tweet'])\n",
    "\n",
    "vector_length = 200\n",
    "train_features = average_embeddings(train_sentences, fasttext_model, vector_length=vector_length)\n",
    "test_features = average_embeddings(test_sentences, fasttext_model, vector_length=vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = load_glove_model(\"embeddings/glove/glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = list(train_tweets_cleaned['tweet'])\n",
    "test_sentences = list(test_tweets_cleaned['tweet'])\n",
    "\n",
    "vector_length = 200\n",
    "train_features = average_embeddings(train_sentences, glove_model, vector_length=vector_length)\n",
    "test_features = average_embeddings(test_sentences, glove_model, vector_length=vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Sentence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(text, vector_length=50):\n",
    "    return_vector = [0] * vector_length\n",
    "    for word in text.split():\n",
    "        index = int(binascii.hexlify(word.encode(\"utf-8\")), 16) % vector_length\n",
    "        return_vector[index] += 1\n",
    "    return return_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_length = 50\n",
    "train_features = list(train_tweets_cleaned['tweet'].apply(sentence_to_vector))\n",
    "test_features = list(test_tweets_cleaned['tweet'].apply(sentence_to_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Prediction Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_tweets_cleaned['sentiment'])\n",
    "test_labels = np.array(test_tweets_cleaned['sentiment'])\n",
    "\n",
    "train_labels = [[0, 1] if label == 1 else [1, 0] for label in train_labels]\n",
    "test_labels = [[0, 1] if label == 1 else [1, 0] for label in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Logistic Regression w/ Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LogisticRegression(C=1e5)\n",
    "lr.fit(train_features, np.array(train_tweets_cleaned['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.array(test_tweets_cleaned['sentiment']), lr.predict(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Neural Network w/ Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=vector_length))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.05, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(train_features), np.array(train_labels), epochs=75, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy > 0.820200\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(model.predict(np.array(test_features), batch_size=128), axis=1)\n",
    "acc_list = [1 if b else 0 for b in np.array(test_tweets_cleaned['sentiment']) == pred]\n",
    "acc = sum(acc_list) / len(acc_list)\n",
    "print(\"Accuracy > %f\"%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_test_tweets = pd.DataFrame(load_file('twitter-dataset/test_data.txt'), columns=['tweet'])\n",
    "kaggle_test_tweets['tweet'] = kaggle_test_tweets['tweet'].apply(lambda tweet: tweet.split(',', 1)[-1])\n",
    "kaggle_test_sentences = list(kaggle_test_tweets['tweet'])\n",
    "kaggle_test_features = average_embeddings(kaggle_test_sentences, glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext Internal\n",
    "pred = fast_text(train_tweets_cleaned, kaggle_test_tweets)\n",
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings\n",
    "pred = np.argmax(model.predict(np.array(kaggle_test_features), batch_size=128), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_file(results,filepath):\n",
    "    with open(filepath, 'w') as file:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writeFile = csv.DictWriter(file, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writeFile.writeheader()\n",
    "        id_ = 1\n",
    "        for result in results:\n",
    "            writeFile.writerow({'Id':int(id_),'Prediction':result})\n",
    "            id_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_file(pred,'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
